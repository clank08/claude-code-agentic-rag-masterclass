# Settings Dialog Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add a settings dialog in the sidebar footer that lets users select their LLM provider and model, persisted per-user in Supabase.

**Architecture:** New `user_settings` table (one row per user, upsert on save). Backend settings router exposes GET/PUT endpoints. Chat router loads user settings and overrides env var defaults when calling `llm_service`. Frontend adds a gear icon in sidebar footer that opens a shadcn Dialog with provider/model dropdowns.

**Tech Stack:** Python/FastAPI, Supabase, React + shadcn/ui (Dialog, Select, Input, Label)

**Design doc:** `docs/plans/2026-02-26-settings-dialog-design.md`

⚠️ **Medium** — 7 tasks across backend + frontend. Straightforward but touches multiple layers.

---

## Phase 1: Database

### Task 1: Create user_settings migration

**Files:**
- Create: `backend/database/007_user_settings.sql`

**Step 1: Create `backend/database/007_user_settings.sql`**

```sql
-- Settings Dialog: User settings table for per-user LLM configuration
-- Run this in Supabase SQL Editor

create table public.user_settings (
    id uuid primary key default uuid_generate_v4(),
    user_id uuid not null unique references auth.users(id) on delete cascade,
    llm_base_url text,
    llm_model text,
    created_at timestamptz not null default now(),
    updated_at timestamptz not null default now()
);

create index idx_user_settings_user_id on public.user_settings(user_id);

alter table public.user_settings enable row level security;

create policy "Users can view their own settings"
    on public.user_settings for select using (auth.uid() = user_id);
create policy "Users can create their own settings"
    on public.user_settings for insert with check (auth.uid() = user_id);
create policy "Users can update their own settings"
    on public.user_settings for update
    using (auth.uid() = user_id) with check (auth.uid() = user_id);

-- Service role needs to read user settings for chat requests
create policy "Service role can read all settings"
    on public.user_settings for select using (true);

create trigger update_user_settings_updated_at
    before update on public.user_settings
    for each row execute function public.update_updated_at_column();
```

**Step 2: Commit**

```bash
git add backend/database/007_user_settings.sql
git commit -m "feat: add user_settings migration for per-user LLM config"
```

**Validate:** User must run this SQL in Supabase SQL Editor manually. Table should appear in Supabase dashboard.

---

## Phase 2: Backend

### Task 2: Add settings schemas

**Files:**
- Modify: `backend/app/models/schemas.py`

**Step 1: Add settings schemas to `backend/app/models/schemas.py`**

Append these classes after the existing `DocumentResponse` class:

```python
class UserSettingsRequest(BaseModel):
    llm_base_url: str | None = None
    llm_model: str | None = None


class UserSettingsResponse(BaseModel):
    llm_base_url: str | None = None
    llm_model: str | None = None


class SettingsDefaultsResponse(BaseModel):
    llm_base_url: str
    llm_model: str
```

**Step 2: Commit**

```bash
git add backend/app/models/schemas.py
git commit -m "feat: add settings Pydantic schemas"
```

**Validate:** `cd backend && source venv/Scripts/activate && python -c "from app.models.schemas import UserSettingsRequest, UserSettingsResponse, SettingsDefaultsResponse; print('OK')"`

---

### Task 3: Create settings router

**Files:**
- Create: `backend/app/routers/settings.py`
- Modify: `backend/app/main.py`

**Step 1: Create `backend/app/routers/settings.py`**

```python
from fastapi import APIRouter, Depends
from app.middleware.auth import get_current_user
from app.models.schemas import (
    AuthenticatedUser,
    UserSettingsRequest,
    UserSettingsResponse,
    SettingsDefaultsResponse,
)
from app.database.supabase_client import supabase
from app.config import settings

router = APIRouter(prefix="/api/settings", tags=["settings"])


@router.get("", response_model=UserSettingsResponse)
async def get_settings(user: AuthenticatedUser = Depends(get_current_user)):
    result = (
        supabase.table("user_settings")
        .select("llm_base_url, llm_model")
        .eq("user_id", user.id)
        .execute()
    )
    if not result.data:
        return UserSettingsResponse()
    return result.data[0]


@router.put("", response_model=UserSettingsResponse)
async def update_settings(
    request: UserSettingsRequest,
    user: AuthenticatedUser = Depends(get_current_user),
):
    # Upsert: insert or update on conflict
    result = (
        supabase.table("user_settings")
        .upsert(
            {
                "user_id": user.id,
                "llm_base_url": request.llm_base_url,
                "llm_model": request.llm_model,
            },
            on_conflict="user_id",
        )
        .execute()
    )
    return result.data[0]


@router.get("/defaults", response_model=SettingsDefaultsResponse)
async def get_defaults(user: AuthenticatedUser = Depends(get_current_user)):
    return SettingsDefaultsResponse(
        llm_base_url=settings.llm_base_url,
        llm_model=settings.llm_model,
    )
```

**Step 2: Register router in `backend/app/main.py`**

Add to the imports:

```python
from app.routers import chat, threads, documents, settings
```

Add after the existing `include_router` calls:

```python
app.include_router(settings.router)
```

**Step 3: Commit**

```bash
git add backend/app/routers/settings.py backend/app/main.py
git commit -m "feat: add settings router with GET/PUT endpoints"
```

**Validate:** Start backend, then `curl -s http://localhost:8000/api/settings/defaults -H "Authorization: Bearer <token>"` should return the server defaults.

---

### Task 4: Wire user settings into chat flow

**Files:**
- Modify: `backend/app/services/llm_service.py`
- Modify: `backend/app/routers/chat.py`

**Step 1: Update `llm_service.py` to accept per-request overrides**

Replace the module-level `client` and `stream_chat_response` function. The key change: accept optional `base_url` and `model` parameters that override the env var defaults.

Replace the entire file with:

```python
import json
from openai import AsyncOpenAI
from langsmith.wrappers import wrap_openai
from app.config import settings

# Default client using env var config
_default_client = wrap_openai(
    AsyncOpenAI(
        base_url=settings.llm_base_url,
        api_key=settings.llm_api_key,
    )
)

# Cache custom clients by base_url to avoid creating new ones per request
_custom_clients: dict[str, AsyncOpenAI] = {}


def _get_client(base_url: str | None) -> AsyncOpenAI:
    """Get an OpenAI client for the given base_url, or the default."""
    if not base_url or base_url == settings.llm_base_url:
        return _default_client
    if base_url not in _custom_clients:
        _custom_clients[base_url] = wrap_openai(
            AsyncOpenAI(
                base_url=base_url,
                api_key=settings.llm_api_key,
            )
        )
    return _custom_clients[base_url]


# Tool definitions for retrieval
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "search_documents",
            "description": "Search the user's uploaded documents for relevant information. Use this when the user asks questions that might be answered by their documents.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query to find relevant document chunks",
                    }
                },
                "required": ["query"],
            },
        },
    }
]


async def stream_chat_response(
    messages: list[dict],
    user_id: str | None = None,
    model: str | None = None,
    base_url: str | None = None,
):
    """Stream a chat completion response.

    Args:
        messages: Full conversation history as Chat Completions messages array.
                  Must include system prompt as first message.
        user_id: For LangSmith trace metadata.
        model: Override model (from user settings). Falls back to env var default.
        base_url: Override base_url (from user settings). Falls back to env var default.

    Yields tuples of (event_type, data):
        - ("text_delta", token_text) — streamed text token
        - ("tool_call", {"id": str, "name": str, "arguments": str}) — tool invocation
        - ("done", None) — stream complete
        - ("error", error_message) — error occurred
    """
    client = _get_client(base_url)
    use_model = model or settings.llm_model

    try:
        stream = await client.chat.completions.create(
            model=use_model,
            messages=messages,
            tools=TOOLS,
            stream=True,
        )

        # Accumulators for tool calls (streamed in chunks)
        tool_calls: dict[int, dict] = {}

        async for chunk in stream:
            delta = chunk.choices[0].delta if chunk.choices else None
            if not delta:
                continue

            finish_reason = chunk.choices[0].finish_reason

            # Text content
            if delta.content:
                yield ("text_delta", delta.content)

            # Tool call chunks
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    idx = tc.index
                    if idx not in tool_calls:
                        tool_calls[idx] = {
                            "id": tc.id or "",
                            "name": tc.function.name or "" if tc.function else "",
                            "arguments": "",
                        }
                    if tc.id:
                        tool_calls[idx]["id"] = tc.id
                    if tc.function:
                        if tc.function.name:
                            tool_calls[idx]["name"] = tc.function.name
                        if tc.function.arguments:
                            tool_calls[idx]["arguments"] += tc.function.arguments

            # Stream finished
            if finish_reason == "tool_calls":
                for idx in sorted(tool_calls.keys()):
                    yield ("tool_call", tool_calls[idx])
                return  # Caller must handle tool execution and re-invoke
            elif finish_reason == "stop":
                yield ("done", None)
                return

        # If we exit the loop without a finish_reason
        yield ("done", None)

    except Exception as e:
        yield ("error", str(e))
```

**Step 2: Update `chat.py` to load user settings before streaming**

Add a helper to load user settings and pass them to `stream_chat_response`. In `backend/app/routers/chat.py`:

At the top, after the existing imports, add:

```python
from app.database.supabase_client import supabase as sb_client
```

Wait — `supabase` is already imported. We just need to load user settings and pass `model`/`base_url` to the stream call.

Add a helper function after `_build_messages`:

```python
def _get_user_llm_settings(user_id: str) -> dict:
    """Load user's LLM settings. Returns dict with model and base_url (may be None)."""
    result = (
        supabase.table("user_settings")
        .select("llm_base_url, llm_model")
        .eq("user_id", user_id)
        .execute()
    )
    if result.data:
        return result.data[0]
    return {}
```

Then update the `stream_chat_response` call inside `event_generator()`. Replace:

```python
            async for event_type, data in stream_chat_response(
                messages=current_messages,
                user_id=user.id,
            ):
```

With:

```python
            async for event_type, data in stream_chat_response(
                messages=current_messages,
                user_id=user.id,
                model=user_llm_settings.get("llm_model"),
                base_url=user_llm_settings.get("llm_base_url"),
            ):
```

And at the start of `event_generator()`, before the `yield {"event": "thread_id"...}` line, add:

```python
        user_llm_settings = _get_user_llm_settings(user.id)
```

**Step 3: Commit**

```bash
git add backend/app/services/llm_service.py backend/app/routers/chat.py
git commit -m "feat: wire user settings into chat flow with per-request model/base_url overrides"
```

**Validate:** Start backend. Chat should still work with default settings (no user_settings row yet).

---

## Phase 3: Frontend

### Task 5: Install shadcn Dialog and Select components

**Files:**
- Create: `frontend/src/components/ui/dialog.tsx` (generated by shadcn)
- Create: `frontend/src/components/ui/select.tsx` (generated by shadcn)

**Step 1: Install components**

```bash
cd frontend && npx shadcn@latest add dialog select -y
```

**Step 2: Commit**

```bash
git add frontend/src/components/ui/dialog.tsx frontend/src/components/ui/select.tsx
git commit -m "feat: add shadcn dialog and select components"
```

**Validate:** Both files exist in `frontend/src/components/ui/`.

---

### Task 6: Create settings dialog component

**Files:**
- Create: `frontend/src/components/settings/settings-dialog.tsx`

**Step 1: Create `frontend/src/components/settings/settings-dialog.tsx`**

```tsx
import { useState, useEffect } from "react";
import { Settings } from "lucide-react";
import { Button } from "@/components/ui/button";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
  DialogTrigger,
} from "@/components/ui/dialog";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { Input } from "@/components/ui/input";
import { Label } from "@/components/ui/label";
import { apiGet, apiPut } from "@/lib/api";

interface UserSettings {
  llm_base_url: string | null;
  llm_model: string | null;
}

interface SettingsDefaults {
  llm_base_url: string;
  llm_model: string;
}

const PROVIDERS = [
  { label: "OpenRouter", value: "https://openrouter.ai/api/v1" },
  { label: "OpenAI", value: "https://api.openai.com/v1" },
  { label: "Custom", value: "__custom__" },
] as const;

const MODELS_BY_PROVIDER: Record<string, { label: string; value: string }[]> = {
  "https://openrouter.ai/api/v1": [
    { label: "Claude Sonnet 4", value: "anthropic/claude-sonnet-4" },
    { label: "GPT-4.1", value: "openai/gpt-4.1" },
    { label: "Gemini 2.5 Flash", value: "google/gemini-2.5-flash" },
    { label: "Custom", value: "__custom__" },
  ],
  "https://api.openai.com/v1": [
    { label: "GPT-4.1", value: "gpt-4.1" },
    { label: "GPT-4.1 Mini", value: "gpt-4.1-mini" },
    { label: "o4-mini", value: "o4-mini" },
    { label: "Custom", value: "__custom__" },
  ],
};

export function SettingsDialog() {
  const [open, setOpen] = useState(false);
  const [defaults, setDefaults] = useState<SettingsDefaults | null>(null);
  const [providerValue, setProviderValue] = useState<string>("");
  const [customBaseUrl, setCustomBaseUrl] = useState("");
  const [modelValue, setModelValue] = useState<string>("");
  const [customModel, setCustomModel] = useState("");
  const [saving, setSaving] = useState(false);

  // Load settings + defaults when dialog opens
  useEffect(() => {
    if (!open) return;

    Promise.all([
      apiGet<UserSettings>("/api/settings"),
      apiGet<SettingsDefaults>("/api/settings/defaults"),
    ]).then(([userSettings, serverDefaults]) => {
      setDefaults(serverDefaults);

      const baseUrl = userSettings.llm_base_url || serverDefaults.llm_base_url;
      const model = userSettings.llm_model || serverDefaults.llm_model;

      // Determine provider selection
      const knownProvider = PROVIDERS.find((p) => p.value === baseUrl);
      if (knownProvider) {
        setProviderValue(baseUrl);
      } else {
        setProviderValue("__custom__");
        setCustomBaseUrl(baseUrl);
      }

      // Determine model selection
      const modelsForProvider = MODELS_BY_PROVIDER[baseUrl];
      const knownModel = modelsForProvider?.find((m) => m.value === model);
      if (knownModel) {
        setModelValue(model);
      } else {
        setModelValue("__custom__");
        setCustomModel(model);
      }
    });
  }, [open]);

  const effectiveBaseUrl =
    providerValue === "__custom__" ? customBaseUrl : providerValue;
  const effectiveModel =
    modelValue === "__custom__" ? customModel : modelValue;

  const modelsForCurrentProvider = MODELS_BY_PROVIDER[effectiveBaseUrl] || [];
  const isCustomProvider = providerValue === "__custom__";

  const handleProviderChange = (value: string) => {
    setProviderValue(value);
    // Reset model when provider changes
    setModelValue("");
    setCustomModel("");
  };

  const handleSave = async () => {
    setSaving(true);
    try {
      // Send null if matching server defaults to mean "use default"
      const baseUrlToSave =
        effectiveBaseUrl === defaults?.llm_base_url ? null : effectiveBaseUrl;
      const modelToSave =
        effectiveModel === defaults?.llm_model ? null : effectiveModel;

      await apiPut("/api/settings", {
        llm_base_url: baseUrlToSave,
        llm_model: modelToSave,
      });
      setOpen(false);
    } finally {
      setSaving(false);
    }
  };

  const handleReset = async () => {
    setSaving(true);
    try {
      await apiPut("/api/settings", {
        llm_base_url: null,
        llm_model: null,
      });
      if (defaults) {
        const knownProvider = PROVIDERS.find(
          (p) => p.value === defaults.llm_base_url
        );
        setProviderValue(knownProvider ? defaults.llm_base_url : "__custom__");
        setCustomBaseUrl(knownProvider ? "" : defaults.llm_base_url);

        const modelsForDefault = MODELS_BY_PROVIDER[defaults.llm_base_url];
        const knownModel = modelsForDefault?.find(
          (m) => m.value === defaults.llm_model
        );
        setModelValue(knownModel ? defaults.llm_model : "__custom__");
        setCustomModel(knownModel ? "" : defaults.llm_model);
      }
    } finally {
      setSaving(false);
    }
  };

  return (
    <Dialog open={open} onOpenChange={setOpen}>
      <DialogTrigger asChild>
        <Button variant="ghost" size="icon">
          <Settings className="h-4 w-4" />
        </Button>
      </DialogTrigger>
      <DialogContent className="sm:max-w-[425px]">
        <DialogHeader>
          <DialogTitle>Model Settings</DialogTitle>
          <DialogDescription>
            Choose your LLM provider and model. Changes apply to new messages.
          </DialogDescription>
        </DialogHeader>
        <div className="grid gap-4 py-4">
          <div className="grid gap-2">
            <Label htmlFor="provider">Provider</Label>
            <Select value={providerValue} onValueChange={handleProviderChange}>
              <SelectTrigger id="provider">
                <SelectValue placeholder="Select provider" />
              </SelectTrigger>
              <SelectContent>
                {PROVIDERS.map((p) => (
                  <SelectItem key={p.value} value={p.value}>
                    {p.label}
                  </SelectItem>
                ))}
              </SelectContent>
            </Select>
            {isCustomProvider && (
              <Input
                placeholder="https://api.example.com/v1"
                value={customBaseUrl}
                onChange={(e) => setCustomBaseUrl(e.target.value)}
              />
            )}
          </div>
          <div className="grid gap-2">
            <Label htmlFor="model">Model</Label>
            {modelsForCurrentProvider.length > 0 ? (
              <Select value={modelValue} onValueChange={setModelValue}>
                <SelectTrigger id="model">
                  <SelectValue placeholder="Select model" />
                </SelectTrigger>
                <SelectContent>
                  {modelsForCurrentProvider.map((m) => (
                    <SelectItem key={m.value} value={m.value}>
                      {m.label}
                    </SelectItem>
                  ))}
                </SelectContent>
              </Select>
            ) : null}
            {(modelValue === "__custom__" ||
              isCustomProvider ||
              modelsForCurrentProvider.length === 0) && (
              <Input
                placeholder={defaults?.llm_model || "model-name"}
                value={customModel}
                onChange={(e) => {
                  setCustomModel(e.target.value);
                  if (modelValue !== "__custom__") setModelValue("__custom__");
                }}
              />
            )}
          </div>
        </div>
        <DialogFooter className="flex justify-between sm:justify-between">
          <Button variant="outline" onClick={handleReset} disabled={saving}>
            Reset to Defaults
          </Button>
          <Button onClick={handleSave} disabled={saving}>
            {saving ? "Saving..." : "Save"}
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
}
```

**Step 2: Commit**

```bash
git add frontend/src/components/settings/settings-dialog.tsx
git commit -m "feat: add settings dialog component with provider/model selection"
```

**Validate:** File compiles without TypeScript errors: `cd frontend && npx tsc --noEmit`

---

### Task 7: Add apiPut helper and wire dialog into sidebar

**Files:**
- Modify: `frontend/src/lib/api.ts`
- Modify: `frontend/src/components/layout/app-sidebar.tsx`

**Step 1: Add `apiPut` to `frontend/src/lib/api.ts`**

Add after the existing `apiPost` function:

```typescript
export async function apiPut<T>(path: string, body?: unknown): Promise<T> {
  const headers = await getAuthHeaders();
  const res = await fetch(`${API_URL}${path}`, {
    method: "PUT",
    headers,
    body: body ? JSON.stringify(body) : undefined,
  });
  if (!res.ok) throw new Error(`API error: ${res.status}`);
  return res.json();
}
```

**Step 2: Add settings dialog to sidebar footer**

In `frontend/src/components/layout/app-sidebar.tsx`:

Add to imports:

```typescript
import { SettingsDialog } from "@/components/settings/settings-dialog";
```

Replace the `<SidebarFooter>` section (the entire block from `<SidebarFooter>` to `</SidebarFooter>`):

```tsx
      <SidebarFooter>
        <Separator />
        <div className="flex items-center justify-between p-2">
          <span className="truncate text-sm text-muted-foreground">{user?.email}</span>
          <div className="flex items-center gap-1">
            <SettingsDialog />
            <Button variant="ghost" size="icon" onClick={signOut}>
              <LogOut className="h-4 w-4" />
            </Button>
          </div>
        </div>
      </SidebarFooter>
```

**Step 3: Commit**

```bash
git add frontend/src/lib/api.ts frontend/src/components/layout/app-sidebar.tsx
git commit -m "feat: wire settings dialog into sidebar footer with gear icon"
```

**Validate:** Start frontend, log in. Gear icon should appear in sidebar footer next to the logout button. Clicking it opens the settings dialog. Saving/resetting should work if the backend is running and the `user_settings` table exists.

---

## Manual Validation

After all tasks are complete:

1. Run the `007_user_settings.sql` migration in Supabase SQL Editor
2. Start backend and frontend
3. Log in, click the gear icon in sidebar footer
4. Select a different provider/model, save
5. Send a chat message — verify it uses the new model (check LangSmith trace)
6. Refresh the page, reopen settings — verify the saved values persist
7. Click "Reset to Defaults" — verify it reverts to server defaults
8. Send another message — verify it uses the default model again
