# Module 2: BYO Retrieval + Memory â€” Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Replace OpenAI Responses API with Chat Completions (any OpenAI-compatible provider), add document ingestion pipeline (upload â†’ chunk â†’ embed â†’ pgvector), and integrate retrieval via tool calling.

**Architecture:** Thin wrapper on OpenAI SDK using configurable `base_url` for provider flexibility. Backend loads full chat history from Supabase per request (stateless). Document ingestion runs as a background task, storing chunks with embeddings in pgvector. Retrieval is exposed as a Chat Completions tool (`search_documents`) that the LLM invokes when it needs document context.

**Tech Stack:** Python/FastAPI, OpenAI SDK (Chat Completions), pgvector, Supabase Storage + Realtime, React + shadcn/ui

**Design doc:** `docs/plans/2026-02-25-module2-byo-retrieval-memory-design.md`

ðŸ”´ **Complex** â€” 6 phases, 16 tasks. Execute phases in order; tasks within a phase can sometimes be parallelized.

---

## Phase 1: LLM Provider Migration

Replace the OpenAI Responses API with Chat Completions API. After this phase, chat works exactly as before but uses any OpenAI-compatible provider.

### Task 1: Update Config & Environment

**Files:**
- Modify: `backend/app/config.py`
- Modify: `backend/.env.example`
- Modify: user's `backend/.env` (manual step)

**Step 1: Update `backend/app/config.py`**

Replace the OpenAI-specific fields with generic LLM fields. Keep Supabase and LangSmith fields unchanged.

```python
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    # LLM Provider (any OpenAI-compatible endpoint)
    llm_api_key: str
    llm_base_url: str = "https://openrouter.ai/api/v1"
    llm_model: str = "anthropic/claude-sonnet-4"
    llm_system_prompt: str = "You are a helpful assistant. Use the search_documents tool to find relevant information from uploaded documents when the user asks about their documents."

    # Embedding
    embedding_api_key: str = ""
    embedding_base_url: str = "https://api.openai.com/v1"
    embedding_model: str = "text-embedding-3-small"
    embedding_dimensions: int = 1536

    # Ingestion
    chunk_size: int = 1000
    chunk_overlap: int = 200

    # Retrieval
    retrieval_top_k: int = 5
    retrieval_score_threshold: float = 0.7

    # Supabase
    supabase_url: str
    supabase_service_role_key: str
    supabase_anon_key: str = ""
    supabase_jwt_secret: str = ""

    # Observability
    langsmith_api_key: str
    langsmith_project: str = "rag-masterclass"

    # App
    frontend_url: str = "http://localhost:5173"

    model_config = {"env_file": ".env", "extra": "ignore"}


settings = Settings()
```

**Step 2: Update `backend/.env.example`**

```
LLM_API_KEY=
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_MODEL=anthropic/claude-sonnet-4
LLM_SYSTEM_PROMPT=You are a helpful assistant. Use the search_documents tool to find relevant information from uploaded documents when the user asks about their documents.
EMBEDDING_API_KEY=
EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVAL_TOP_K=5
RETRIEVAL_SCORE_THRESHOLD=0.7
SUPABASE_URL=
SUPABASE_SERVICE_ROLE_KEY=
SUPABASE_JWT_SECRET=
LANGSMITH_API_KEY=
LANGSMITH_PROJECT=rag-masterclass
FRONTEND_URL=http://localhost:5173
```

**Step 3: Update your `backend/.env`**

Manual step â€” update your actual `.env` to use the new variable names. Map:
- `OPENAI_API_KEY` â†’ `LLM_API_KEY` (use your OpenRouter key, or keep OpenAI key with `LLM_BASE_URL=https://api.openai.com/v1`)
- `OPENAI_MODEL` â†’ `LLM_MODEL`
- Remove `OPENAI_VECTOR_STORE_ID`
- Add `EMBEDDING_API_KEY` (can be same as your OpenAI key)

**Validate:** `cd backend && python -c "from app.config import settings; print(settings.llm_base_url, settings.llm_model)"` prints your configured values.

**Step 4: Commit**

```bash
git add backend/app/config.py backend/.env.example
git commit -m "feat: replace OpenAI config with generic LLM provider settings"
```

---

### Task 2: Rewrite LLM Service (Responses API â†’ Chat Completions)

**Files:**
- Delete: `backend/app/services/openai_service.py`
- Create: `backend/app/services/llm_service.py`
- Modify: `backend/app/services/langsmith_service.py` (no changes needed, but verify `wrap_openai` still works)

**Step 1: Create `backend/app/services/llm_service.py`**

This replaces `openai_service.py`. Uses Chat Completions API with streaming. Handles tool calls by yielding them back to the caller (the chat router will execute tools and re-invoke).

```python
import json
from openai import AsyncOpenAI
from langsmith.wrappers import wrap_openai
from app.config import settings

client = wrap_openai(
    AsyncOpenAI(
        base_url=settings.llm_base_url,
        api_key=settings.llm_api_key,
    )
)

# Tool definitions for retrieval
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "search_documents",
            "description": "Search the user's uploaded documents for relevant information. Use this when the user asks questions that might be answered by their documents.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query to find relevant document chunks",
                    }
                },
                "required": ["query"],
            },
        },
    }
]


async def stream_chat_response(messages: list[dict], user_id: str | None = None):
    """Stream a chat completion response.

    Args:
        messages: Full conversation history as Chat Completions messages array.
                  Must include system prompt as first message.
        user_id: For LangSmith trace metadata.

    Yields tuples of (event_type, data):
        - ("text_delta", token_text) â€” streamed text token
        - ("tool_call", {"id": str, "name": str, "arguments": str}) â€” tool invocation
        - ("done", None) â€” stream complete
        - ("error", error_message) â€” error occurred
    """
    try:
        stream = await client.chat.completions.create(
            model=settings.llm_model,
            messages=messages,
            tools=TOOLS,
            stream=True,
        )

        # Accumulators for tool calls (streamed in chunks)
        tool_calls: dict[int, dict] = {}

        async for chunk in stream:
            delta = chunk.choices[0].delta if chunk.choices else None
            if not delta:
                continue

            finish_reason = chunk.choices[0].finish_reason

            # Text content
            if delta.content:
                yield ("text_delta", delta.content)

            # Tool call chunks
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    idx = tc.index
                    if idx not in tool_calls:
                        tool_calls[idx] = {
                            "id": tc.id or "",
                            "name": tc.function.name or "" if tc.function else "",
                            "arguments": "",
                        }
                    if tc.id:
                        tool_calls[idx]["id"] = tc.id
                    if tc.function:
                        if tc.function.name:
                            tool_calls[idx]["name"] = tc.function.name
                        if tc.function.arguments:
                            tool_calls[idx]["arguments"] += tc.function.arguments

            # Stream finished
            if finish_reason == "tool_calls":
                for idx in sorted(tool_calls.keys()):
                    yield ("tool_call", tool_calls[idx])
                return  # Caller must handle tool execution and re-invoke
            elif finish_reason == "stop":
                yield ("done", None)
                return

        # If we exit the loop without a finish_reason
        yield ("done", None)

    except Exception as e:
        yield ("error", str(e))
```

**Step 2: Delete `backend/app/services/openai_service.py`**

```bash
rm backend/app/services/openai_service.py
```

**Validate:** `cd backend && python -c "from app.services.llm_service import stream_chat_response; print('OK')"` â€” imports without error.

**Step 3: Commit**

```bash
git add backend/app/services/llm_service.py
git rm backend/app/services/openai_service.py
git commit -m "feat: replace Responses API with Chat Completions in llm_service"
```

---

### Task 3: Update Chat Router

**Files:**
- Modify: `backend/app/routers/chat.py`
- Modify: `backend/app/main.py` (update import)

**Step 1: Rewrite `backend/app/routers/chat.py`**

Key changes:
- Load full message history from `messages` table before each LLM call
- Prepend system prompt
- Handle tool call loop (execute search, send result back, continue)
- Remove all `openai_response_id` / `previous_response_id` logic
- `done` event no longer includes `response_id`

```python
import json
from fastapi import APIRouter, Depends, HTTPException
from sse_starlette.sse import EventSourceResponse
from app.middleware.auth import get_current_user
from app.models.schemas import AuthenticatedUser, ChatRequest
from app.database.supabase_client import supabase
from app.services.llm_service import stream_chat_response
from app.config import settings

router = APIRouter(prefix="/api", tags=["chat"])


def _build_messages(system_prompt: str, history: list[dict]) -> list[dict]:
    """Build Chat Completions messages array from system prompt + history."""
    messages = [{"role": "system", "content": system_prompt}]
    for msg in history:
        messages.append({"role": msg["role"], "content": msg["content"]})
    return messages


@router.post("/chat")
async def chat(
    request: ChatRequest, user: AuthenticatedUser = Depends(get_current_user)
):
    thread_id = request.thread_id

    if thread_id:
        # Verify thread belongs to user
        result = (
            supabase.table("threads")
            .select("id")
            .eq("id", thread_id)
            .eq("user_id", user.id)
            .execute()
        )
        if not result.data:
            raise HTTPException(status_code=404, detail="Thread not found")
    else:
        # Auto-create thread with message preview as title
        title = request.message[:50] + ("..." if len(request.message) > 50 else "")
        result = (
            supabase.table("threads")
            .insert({"user_id": user.id, "title": title})
            .execute()
        )
        thread_id = result.data[0]["id"]

    # Save user message
    supabase.table("messages").insert(
        {
            "thread_id": thread_id,
            "user_id": user.id,
            "role": "user",
            "content": request.message,
        }
    ).execute()

    # Load full conversation history for this thread
    history_result = (
        supabase.table("messages")
        .select("role, content")
        .eq("thread_id", thread_id)
        .eq("user_id", user.id)
        .order("created_at")
        .execute()
    )
    messages = _build_messages(settings.llm_system_prompt, history_result.data)

    async def event_generator():
        yield {"event": "thread_id", "data": json.dumps({"thread_id": thread_id})}

        full_content = ""
        current_messages = list(messages)  # Copy for tool call loop

        # Tool call loop â€” LLM may call tools, we execute and re-invoke
        max_tool_rounds = 5
        for _ in range(max_tool_rounds):
            tool_calls_received = []
            round_content = ""

            async for event_type, data in stream_chat_response(
                messages=current_messages,
                user_id=user.id,
            ):
                if event_type == "text_delta":
                    round_content += data
                    full_content += data
                    yield {
                        "event": "text_delta",
                        "data": json.dumps({"token": data}),
                    }
                elif event_type == "tool_call":
                    tool_calls_received.append(data)
                elif event_type == "done":
                    # Save assistant message
                    supabase.table("messages").insert(
                        {
                            "thread_id": thread_id,
                            "user_id": user.id,
                            "role": "assistant",
                            "content": full_content,
                        }
                    ).execute()
                    # Update thread updated_at
                    supabase.table("threads").update(
                        {"title": supabase.table("threads").select("title").eq("id", thread_id).execute().data[0]["title"]}
                    ).eq("id", thread_id).eq("user_id", user.id).execute()
                    yield {
                        "event": "done",
                        "data": json.dumps({"thread_id": thread_id}),
                    }
                    return
                elif event_type == "error":
                    yield {
                        "event": "error",
                        "data": json.dumps({"error": data}),
                    }
                    return

            # If we got tool calls, execute them and loop
            if tool_calls_received:
                # Append assistant message with tool calls to context
                current_messages.append(
                    {
                        "role": "assistant",
                        "content": round_content or None,
                        "tool_calls": [
                            {
                                "id": tc["id"],
                                "type": "function",
                                "function": {
                                    "name": tc["name"],
                                    "arguments": tc["arguments"],
                                },
                            }
                            for tc in tool_calls_received
                        ],
                    }
                )

                # Execute each tool call
                for tc in tool_calls_received:
                    tool_result = await _execute_tool_call(
                        tc["name"], tc["arguments"], user.id
                    )
                    current_messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tc["id"],
                            "content": tool_result,
                        }
                    )
                # Continue loop â€” send tool results back to LLM
            else:
                # No tool calls and no done event â€” shouldn't happen, but bail
                break

    return EventSourceResponse(
        event_generator(),
        headers={
            "X-Accel-Buffering": "no",
            "Cache-Control": "no-cache",
        },
    )


async def _execute_tool_call(name: str, arguments: str, user_id: str) -> str:
    """Execute a tool call and return the result as a string."""
    try:
        args = json.loads(arguments)
    except json.JSONDecodeError:
        return json.dumps({"error": "Invalid tool arguments"})

    if name == "search_documents":
        # Import here to avoid circular imports (retrieval_service depends on embedding_service)
        from app.services.retrieval_service import search_documents

        query = args.get("query", "")
        results = await search_documents(query=query, user_id=user_id)
        if not results:
            return json.dumps({"results": [], "message": "No relevant documents found."})
        return json.dumps({"results": results})
    else:
        return json.dumps({"error": f"Unknown tool: {name}"})
```

**Note:** The `_execute_tool_call` function imports `retrieval_service` which doesn't exist yet. This is fine â€” tool calling won't trigger until documents are uploaded (Task 11+). For now, if somehow called, it will fail gracefully.

**Step 2: Update `backend/app/main.py`**

Change the import and exception handler:

```python
import os
from app.config import settings

# Set LangSmith env vars before other imports
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = settings.langsmith_api_key
os.environ["LANGSMITH_PROJECT"] = settings.langsmith_project

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from openai import OpenAIError
from app.routers import chat, threads

app = FastAPI(title="RAG Masterclass API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[settings.frontend_url],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(chat.router)
app.include_router(threads.router)


@app.get("/api/health")
async def health():
    return {"status": "ok"}


@app.exception_handler(OpenAIError)
async def openai_exception_handler(request: Request, exc: OpenAIError):
    return JSONResponse(
        status_code=502,
        content={"detail": f"LLM provider error: {str(exc)}"},
    )
```

The `main.py` changes are minimal â€” just the error message wording. The import path change from `openai_service` to `llm_service` is in `chat.py`.

**Validate:** Start the backend: `cd backend && source venv/Scripts/activate && uvicorn app.main:app --reload --port 8000`. Hit health endpoint: `curl http://localhost:8000/api/health` â†’ `{"status":"ok"}`.

**Step 3: Commit**

```bash
git add backend/app/routers/chat.py backend/app/main.py
git commit -m "feat: update chat router for Chat Completions with tool call loop"
```

---

### Task 4: Update Frontend SSE Handler

**Files:**
- Modify: `frontend/src/lib/sse.ts`

**Step 1: Update SSE callback types**

The `onDone` callback no longer receives `response_id`. Update the type:

```typescript
interface SSECallbacks {
  onToken: (token: string) => void;
  onDone: (data: { thread_id: string }) => void;
  onThreadId: (threadId: string) => void;
  onError: (error: string) => void;
}
```

The rest of the SSE parsing logic stays the same â€” the backend still sends `text_delta`, `done`, `thread_id`, and `error` events.

**Validate:** `cd frontend && npx tsc --noEmit` â€” no type errors.

**Step 2: Commit**

```bash
git add frontend/src/lib/sse.ts
git commit -m "feat: remove response_id from SSE done event type"
```

---

### Task 5: Database Migration â€” Drop openai_response_id

**Files:**
- Create: `backend/database/003_drop_openai_response_id.sql`
- Modify: `backend/app/routers/threads.py` (remove any response_id references if present in select)

**Step 1: Create `backend/database/003_drop_openai_response_id.sql`**

```sql
-- Module 2: Remove OpenAI Responses API column from threads table
-- Run this in Supabase SQL Editor

ALTER TABLE public.threads DROP COLUMN IF EXISTS openai_response_id;
```

**Step 2: Run in Supabase SQL Editor**

Manual step â€” run the SQL above in your Supabase dashboard SQL Editor.

**Step 3: Verify `threads.py` has no response_id references**

Check `backend/app/routers/threads.py`. The current code uses `select("*")` and `select("id")` â€” neither explicitly references `openai_response_id`. The `select("*")` will simply stop returning it after the column is dropped. No changes needed.

**Validate:** After running the SQL, check the threads table in Supabase Table Editor â€” the `openai_response_id` column should be gone.

**Step 4: Commit**

```bash
git add backend/database/003_drop_openai_response_id.sql
git commit -m "feat: drop openai_response_id column from threads table"
```

---

### Task 6: End-to-End Chat Validation

**No files to change â€” this is a manual test.**

**Step 1: Start backend**

```bash
cd backend && source venv/Scripts/activate && uvicorn app.main:app --reload --port 8000
```

**Step 2: Start frontend**

```bash
cd frontend && npm run dev
```

**Step 3: Test in browser**

1. Log in
2. Send a message from the empty state (auto-creates thread)
3. Verify tokens stream in real-time
4. Verify message appears in sidebar with title
5. Send a follow-up message â€” verify conversation history is maintained
6. Refresh the page â€” verify messages load from database
7. Check LangSmith dashboard â€” verify traces appear

**Step 4: Verify with a different provider (optional)**

If you have OpenRouter set up, verify the same flow works. If using OpenAI directly, set `LLM_BASE_URL=https://api.openai.com/v1` and `LLM_MODEL=gpt-4.1`.

**Expected:** Chat works identically to Module 1, but now using Chat Completions API. No visible difference to the user.

---

## Phase 2: Database & Storage Setup

New tables and Supabase Storage bucket for document ingestion.

### Task 7: Documents Table

**Files:**
- Create: `backend/database/004_documents.sql`

**Step 1: Create `backend/database/004_documents.sql`**

```sql
-- Module 2: Documents table for tracking uploaded files
-- Run this in Supabase SQL Editor

create table public.documents (
    id uuid primary key default uuid_generate_v4(),
    user_id uuid not null references auth.users(id) on delete cascade,
    filename text not null,
    storage_path text not null,
    mime_type text not null,
    file_size bigint not null,
    status text not null default 'pending'
        check (status in ('pending', 'processing', 'completed', 'failed')),
    error_message text,
    chunk_count integer default 0,
    created_at timestamptz not null default now(),
    updated_at timestamptz not null default now()
);

create index idx_documents_user_id on public.documents(user_id);

alter table public.documents enable row level security;

create policy "Users can view their own documents"
    on public.documents for select using (auth.uid() = user_id);
create policy "Users can create their own documents"
    on public.documents for insert with check (auth.uid() = user_id);
create policy "Users can update their own documents"
    on public.documents for update
    using (auth.uid() = user_id) with check (auth.uid() = user_id);
create policy "Users can delete their own documents"
    on public.documents for delete using (auth.uid() = user_id);

-- Reuse the update_updated_at_column function from 001_threads.sql
create trigger update_documents_updated_at
    before update on public.documents
    for each row execute function public.update_updated_at_column();
```

**Step 2: Run in Supabase SQL Editor**

Manual step.

**Step 3: Enable Realtime on documents table**

In Supabase Dashboard â†’ Database â†’ Replication, enable Realtime for the `documents` table. This is needed for the frontend to receive live status updates.

**Validate:** Table visible in Supabase Table Editor with RLS shield icon active. Realtime enabled.

**Step 4: Commit**

```bash
git add backend/database/004_documents.sql
git commit -m "feat: add documents table with RLS and realtime"
```

---

### Task 8: Chunks Table with pgvector

**Files:**
- Create: `backend/database/005_chunks.sql`

**Step 1: Create `backend/database/005_chunks.sql`**

```sql
-- Module 2: Chunks table with pgvector embeddings
-- Run this in Supabase SQL Editor

-- Enable pgvector extension (Supabase has it pre-installed)
create extension if not exists vector;

create table public.chunks (
    id uuid primary key default uuid_generate_v4(),
    document_id uuid not null references public.documents(id) on delete cascade,
    user_id uuid not null references auth.users(id) on delete cascade,
    content text not null,
    chunk_index integer not null,
    embedding vector(1536),
    metadata jsonb default '{}',
    created_at timestamptz not null default now()
);

create index idx_chunks_document_id on public.chunks(document_id);
create index idx_chunks_user_id on public.chunks(user_id);

-- IVFFlat index for approximate nearest neighbor search
-- Note: This index needs data to work well. With <1000 rows, exact search is fine.
-- The index will be used automatically by Postgres when querying with <=> operator.
-- lists=100 is a good starting point; tune based on data size.
create index idx_chunks_embedding on public.chunks
    using ivfflat (embedding vector_cosine_ops) with (lists = 100);

alter table public.chunks enable row level security;

create policy "Users can view their own chunks"
    on public.chunks for select using (auth.uid() = user_id);
create policy "Users can create their own chunks"
    on public.chunks for insert with check (auth.uid() = user_id);
create policy "Users can delete their own chunks"
    on public.chunks for delete using (auth.uid() = user_id);
```

**Step 2: Run in Supabase SQL Editor**

Manual step.

**Validate:** Both tables exist. Run `select * from chunks limit 1;` â€” should return empty result with correct columns.

**Step 3: Commit**

```bash
git add backend/database/005_chunks.sql
git commit -m "feat: add chunks table with pgvector embedding index"
```

---

### Task 9: Supabase Storage Bucket

**No code files â€” manual setup in Supabase Dashboard.**

**Step 1: Create storage bucket**

In Supabase Dashboard â†’ Storage â†’ Create bucket:
- Name: `documents`
- Public: **No** (private â€” access via service role key only)
- File size limit: 50MB
- Allowed MIME types: `text/plain, text/markdown, text/html, application/pdf, application/vnd.openxmlformats-officedocument.wordprocessingml.document` (plain text for now, others for Module 5)

**Validate:** Bucket visible in Supabase Storage dashboard.

---

## Phase 3: Ingestion Pipeline

Backend services for embedding, chunking, and document processing.

### Task 10: Embedding Service

**Files:**
- Create: `backend/app/services/embedding_service.py`

**Step 1: Create `backend/app/services/embedding_service.py`**

Uses the OpenAI SDK with a separate base_url/api_key for embeddings (can be same or different provider as the LLM).

```python
from openai import AsyncOpenAI
from app.config import settings

# Separate client for embeddings (can point to a different provider than LLM)
_client = AsyncOpenAI(
    base_url=settings.embedding_base_url,
    api_key=settings.embedding_api_key,
)


async def generate_embeddings(texts: list[str]) -> list[list[float]]:
    """Generate embeddings for a list of texts.

    Args:
        texts: List of text strings to embed. Max recommended batch size ~100.

    Returns:
        List of embedding vectors, one per input text.
    """
    if not texts:
        return []

    response = await _client.embeddings.create(
        model=settings.embedding_model,
        input=texts,
        dimensions=settings.embedding_dimensions,
    )

    # Sort by index to preserve input order
    sorted_data = sorted(response.data, key=lambda x: x.index)
    return [item.embedding for item in sorted_data]


async def generate_embedding(text: str) -> list[float]:
    """Generate a single embedding vector for a text string."""
    embeddings = await generate_embeddings([text])
    return embeddings[0]
```

**Validate:** `cd backend && python -c "from app.services.embedding_service import generate_embedding; print('OK')"` â€” imports without error.

**Step 2: Commit**

```bash
git add backend/app/services/embedding_service.py
git commit -m "feat: add embedding service with configurable provider"
```

---

### Task 11: Ingestion Service

**Files:**
- Create: `backend/app/services/ingestion_service.py`

**Step 1: Create `backend/app/services/ingestion_service.py`**

Handles the full pipeline: download file â†’ extract text â†’ chunk â†’ embed â†’ store in pgvector.

```python
import asyncio
from app.config import settings
from app.database.supabase_client import supabase
from app.services.embedding_service import generate_embeddings


def chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> list[str]:
    """Split text into overlapping chunks.

    Args:
        text: The full text to chunk.
        chunk_size: Max characters per chunk.
        chunk_overlap: Number of characters to overlap between chunks.

    Returns:
        List of text chunks.
    """
    if not text.strip():
        return []

    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        start += chunk_size - chunk_overlap

    return chunks


async def process_document(document_id: str, user_id: str):
    """Process a document: download, chunk, embed, store.

    This is the main ingestion pipeline. Call via asyncio.create_task().
    Updates the document status in the database as it progresses.

    Args:
        document_id: UUID of the document row.
        user_id: UUID of the owning user.
    """
    try:
        # Update status to processing
        supabase.table("documents").update({"status": "processing"}).eq(
            "id", document_id
        ).execute()

        # Fetch document metadata
        doc_result = (
            supabase.table("documents")
            .select("*")
            .eq("id", document_id)
            .execute()
        )
        if not doc_result.data:
            raise ValueError(f"Document {document_id} not found")
        doc = doc_result.data[0]

        # Download file from Supabase Storage
        file_bytes = supabase.storage.from_("documents").download(doc["storage_path"])
        text = file_bytes.decode("utf-8")

        # Chunk the text
        chunks = chunk_text(
            text,
            chunk_size=settings.chunk_size,
            chunk_overlap=settings.chunk_overlap,
        )

        if not chunks:
            supabase.table("documents").update(
                {"status": "completed", "chunk_count": 0}
            ).eq("id", document_id).execute()
            return

        # Generate embeddings in batches
        batch_size = 100
        all_embeddings = []
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]
            embeddings = await generate_embeddings(batch)
            all_embeddings.extend(embeddings)

        # Insert chunks with embeddings
        chunk_rows = [
            {
                "document_id": document_id,
                "user_id": user_id,
                "content": chunk,
                "chunk_index": idx,
                "embedding": embedding,
                "metadata": {"filename": doc["filename"]},
            }
            for idx, (chunk, embedding) in enumerate(zip(chunks, all_embeddings))
        ]

        # Insert in batches (Supabase has row limits per request)
        insert_batch_size = 500
        for i in range(0, len(chunk_rows), insert_batch_size):
            batch = chunk_rows[i : i + insert_batch_size]
            supabase.table("chunks").insert(batch).execute()

        # Update document status
        supabase.table("documents").update(
            {"status": "completed", "chunk_count": len(chunks)}
        ).eq("id", document_id).execute()

    except Exception as e:
        # Mark document as failed
        supabase.table("documents").update(
            {"status": "failed", "error_message": str(e)[:500]}
        ).eq("id", document_id).execute()
```

**Validate:** `cd backend && python -c "from app.services.ingestion_service import chunk_text; print(chunk_text('hello world this is a test', 10, 3))"` â€” prints chunked text.

**Step 2: Commit**

```bash
git add backend/app/services/ingestion_service.py
git commit -m "feat: add ingestion service with chunking and embedding pipeline"
```

---

### Task 12: Retrieval Service

**Files:**
- Create: `backend/app/services/retrieval_service.py`

**Step 1: Create `backend/app/services/retrieval_service.py`**

Performs vector similarity search against the chunks table.

```python
from app.config import settings
from app.database.supabase_client import supabase
from app.services.embedding_service import generate_embedding


async def search_documents(
    query: str,
    user_id: str,
    top_k: int | None = None,
    score_threshold: float | None = None,
) -> list[dict]:
    """Search user's document chunks by vector similarity.

    Args:
        query: The search query text.
        user_id: UUID of the user (for RLS-like filtering).
        top_k: Max results to return. Defaults to settings.retrieval_top_k.
        score_threshold: Min similarity score (0-1). Defaults to settings.retrieval_score_threshold.

    Returns:
        List of dicts with keys: content, score, metadata, document_id, chunk_index
    """
    top_k = top_k or settings.retrieval_top_k
    score_threshold = score_threshold or settings.retrieval_score_threshold

    # Generate embedding for the query
    query_embedding = await generate_embedding(query)

    # Use Supabase RPC for vector similarity search
    # This requires a Postgres function â€” we'll create it in the migration
    result = supabase.rpc(
        "match_chunks",
        {
            "query_embedding": query_embedding,
            "match_count": top_k,
            "filter_user_id": user_id,
            "min_similarity": score_threshold,
        },
    ).execute()

    return [
        {
            "content": row["content"],
            "score": row["similarity"],
            "metadata": row["metadata"],
            "document_id": row["document_id"],
            "chunk_index": row["chunk_index"],
        }
        for row in (result.data or [])
    ]
```

**Step 2: Add the `match_chunks` Postgres function**

Create `backend/database/006_match_chunks.sql`:

```sql
-- Module 2: Vector similarity search function for retrieval
-- Run this in Supabase SQL Editor

create or replace function match_chunks(
    query_embedding vector(1536),
    match_count int default 5,
    filter_user_id uuid default null,
    min_similarity float default 0.7
)
returns table (
    id uuid,
    document_id uuid,
    content text,
    chunk_index int,
    metadata jsonb,
    similarity float
)
language plpgsql
as $$
begin
    return query
    select
        c.id,
        c.document_id,
        c.content,
        c.chunk_index,
        c.metadata,
        1 - (c.embedding <=> query_embedding) as similarity
    from chunks c
    where c.user_id = filter_user_id
        and 1 - (c.embedding <=> query_embedding) >= min_similarity
    order by c.embedding <=> query_embedding
    limit match_count;
end;
$$;
```

**Step 3: Run in Supabase SQL Editor**

Manual step.

**Validate:** `cd backend && python -c "from app.services.retrieval_service import search_documents; print('OK')"` â€” imports without error.

**Step 4: Commit**

```bash
git add backend/app/services/retrieval_service.py backend/database/006_match_chunks.sql
git commit -m "feat: add retrieval service with vector similarity search"
```

---

### Task 13: Documents Router

**Files:**
- Create: `backend/app/routers/documents.py`
- Modify: `backend/app/models/schemas.py` (add document schemas)
- Modify: `backend/app/main.py` (register router)

**Step 1: Add document schemas to `backend/app/models/schemas.py`**

Add these schemas alongside the existing ones:

```python
class DocumentResponse(BaseModel):
    id: str
    filename: str
    mime_type: str
    file_size: int
    status: str
    error_message: str | None = None
    chunk_count: int
    created_at: datetime
    updated_at: datetime
```

**Step 2: Create `backend/app/routers/documents.py`**

```python
import asyncio
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from app.middleware.auth import get_current_user
from app.models.schemas import AuthenticatedUser, DocumentResponse
from app.database.supabase_client import supabase
from app.services.ingestion_service import process_document

router = APIRouter(prefix="/api/documents", tags=["documents"])

# Allowed MIME types (plain text for Module 2; PDF/DOCX/HTML added in Module 5)
ALLOWED_MIME_TYPES = {
    "text/plain",
    "text/markdown",
    "text/html",
}

MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB


@router.post("", response_model=DocumentResponse)
async def upload_document(
    file: UploadFile = File(...),
    user: AuthenticatedUser = Depends(get_current_user),
):
    # Validate MIME type
    if file.content_type not in ALLOWED_MIME_TYPES:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type: {file.content_type}. Allowed: {', '.join(ALLOWED_MIME_TYPES)}",
        )

    # Read file content
    content = await file.read()
    if len(content) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="File too large (max 50MB)")

    # Create document record
    doc_result = (
        supabase.table("documents")
        .insert(
            {
                "user_id": user.id,
                "filename": file.filename,
                "storage_path": "",  # Will update after storage upload
                "mime_type": file.content_type,
                "file_size": len(content),
            }
        )
        .execute()
    )
    doc = doc_result.data[0]
    document_id = doc["id"]

    # Upload to Supabase Storage
    storage_path = f"{user.id}/{document_id}/{file.filename}"
    supabase.storage.from_("documents").upload(
        path=storage_path,
        file=content,
        file_options={"content-type": file.content_type},
    )

    # Update storage path
    supabase.table("documents").update({"storage_path": storage_path}).eq(
        "id", document_id
    ).execute()

    # Kick off background processing
    asyncio.create_task(process_document(document_id, user.id))

    # Return the document (re-fetch to get updated storage_path)
    result = (
        supabase.table("documents").select("*").eq("id", document_id).execute()
    )
    return result.data[0]


@router.get("", response_model=list[DocumentResponse])
async def list_documents(user: AuthenticatedUser = Depends(get_current_user)):
    result = (
        supabase.table("documents")
        .select("*")
        .eq("user_id", user.id)
        .order("created_at", desc=True)
        .execute()
    )
    return result.data


@router.delete("/{document_id}")
async def delete_document(
    document_id: str, user: AuthenticatedUser = Depends(get_current_user)
):
    # Fetch document to get storage path
    doc_result = (
        supabase.table("documents")
        .select("*")
        .eq("id", document_id)
        .eq("user_id", user.id)
        .execute()
    )
    if not doc_result.data:
        raise HTTPException(status_code=404, detail="Document not found")

    doc = doc_result.data[0]

    # Delete from Supabase Storage
    try:
        supabase.storage.from_("documents").remove([doc["storage_path"]])
    except Exception:
        pass  # Storage deletion is best-effort

    # Delete document row (cascade deletes chunks)
    supabase.table("documents").delete().eq("id", document_id).eq(
        "user_id", user.id
    ).execute()

    return {"status": "deleted"}
```

**Step 3: Register router in `backend/app/main.py`**

Add to the imports and router registration:

```python
from app.routers import chat, threads, documents

# ... existing code ...

app.include_router(chat.router)
app.include_router(threads.router)
app.include_router(documents.router)
```

**Validate:** Start backend. `curl http://localhost:8000/docs` â€” Swagger UI shows `/api/documents` endpoints.

**Step 4: Commit**

```bash
git add backend/app/routers/documents.py backend/app/models/schemas.py backend/app/main.py
git commit -m "feat: add documents router with upload, list, and delete endpoints"
```

---

## Phase 4: Frontend â€” Navigation & Documents Page

### Task 14: Add Navigation Between Chat and Documents

**Files:**
- Modify: `frontend/src/components/layout/app-sidebar.tsx`
- Modify: `frontend/src/router.tsx`

**Step 1: Update `frontend/src/router.tsx`**

Add the documents route. Import the new page (created in next task):

```typescript
import { BrowserRouter, Routes, Route, Navigate } from "react-router-dom";
import { useAuth } from "@/contexts/auth-context";
import { LoginPage } from "@/pages/login";
import { SignupPage } from "@/pages/signup";
import { ChatPage } from "@/pages/chat";
import { DocumentsPage } from "@/pages/documents";
import { AppLayout } from "@/components/layout/app-layout";
import type { ReactNode } from "react";

function ProtectedRoute({ children }: { children: ReactNode }) {
  const { session, loading } = useAuth();
  if (loading) return null;
  if (!session) return <Navigate to="/login" replace />;
  return <>{children}</>;
}

function PublicRoute({ children }: { children: ReactNode }) {
  const { session, loading } = useAuth();
  if (loading) return null;
  if (session) return <Navigate to="/" replace />;
  return <>{children}</>;
}

export function AppRouter() {
  return (
    <BrowserRouter>
      <Routes>
        <Route path="/login" element={<PublicRoute><LoginPage /></PublicRoute>} />
        <Route path="/signup" element={<PublicRoute><SignupPage /></PublicRoute>} />
        <Route path="/" element={<ProtectedRoute><AppLayout><ChatPage /></AppLayout></ProtectedRoute>} />
        <Route path="/thread/:threadId" element={<ProtectedRoute><AppLayout><ChatPage /></AppLayout></ProtectedRoute>} />
        <Route path="/documents" element={<ProtectedRoute><AppLayout><DocumentsPage /></AppLayout></ProtectedRoute>} />
      </Routes>
    </BrowserRouter>
  );
}
```

**Step 2: Update `frontend/src/components/layout/app-sidebar.tsx`**

Add navigation links for Chat and Documents above the thread list. Add `FileText` icon from lucide-react:

```typescript
import { useNavigate, useParams, useLocation } from "react-router-dom";
import { Plus, Trash2, LogOut, MessageSquare, FileText } from "lucide-react";
```

Add a navigation group at the top of the sidebar content (between `SidebarHeader` and the Conversations group):

```tsx
<SidebarContent>
  <SidebarGroup>
    <SidebarGroupContent>
      <SidebarMenu>
        <SidebarMenuItem>
          <SidebarMenuButton
            isActive={location.pathname === "/" || location.pathname.startsWith("/thread")}
            onClick={() => navigate("/")}
          >
            <MessageSquare className="h-4 w-4" />
            <span>Chat</span>
          </SidebarMenuButton>
        </SidebarMenuItem>
        <SidebarMenuItem>
          <SidebarMenuButton
            isActive={location.pathname === "/documents"}
            onClick={() => navigate("/documents")}
          >
            <FileText className="h-4 w-4" />
            <span>Documents</span>
          </SidebarMenuButton>
        </SidebarMenuItem>
      </SidebarMenu>
    </SidebarGroupContent>
  </SidebarGroup>
  <Separator />
  {/* Existing Conversations group below */}
```

Get `location` from `useLocation()` at the top of the component.

**Validate:** `cd frontend && npx tsc --noEmit` â€” no type errors (the DocumentsPage import will fail until Task 15, that's expected â€” create a stub first or do Task 14+15 together).

**Step 3: Commit**

```bash
git add frontend/src/router.tsx frontend/src/components/layout/app-sidebar.tsx
git commit -m "feat: add navigation between Chat and Documents pages"
```

---

### Task 15: Documents Page and Components

**Files:**
- Create: `frontend/src/pages/documents.tsx`
- Create: `frontend/src/hooks/use-documents.ts`
- Create: `frontend/src/components/documents/file-upload-zone.tsx`
- Create: `frontend/src/components/documents/document-list.tsx`
- Modify: `frontend/src/types/index.ts` (add Document type)
- Modify: `frontend/src/lib/api.ts` (add multipart upload helper)

**Step 1: Add Document type to `frontend/src/types/index.ts`**

```typescript
export interface Document {
  id: string;
  filename: string;
  mime_type: string;
  file_size: number;
  status: "pending" | "processing" | "completed" | "failed";
  error_message: string | null;
  chunk_count: number;
  created_at: string;
  updated_at: string;
}
```

**Step 2: Add upload helper to `frontend/src/lib/api.ts`**

Add this function alongside the existing helpers:

```typescript
export async function apiUpload<T>(path: string, file: File): Promise<T> {
  const { data: { session } } = await supabase.auth.getSession();
  if (!session) throw new Error("Not authenticated");

  const formData = new FormData();
  formData.append("file", file);

  const res = await fetch(`${API_URL}${path}`, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${session.access_token}`,
      // Don't set Content-Type â€” browser sets it with boundary for FormData
    },
    body: formData,
  });
  if (!res.ok) {
    const error = await res.json().catch(() => ({ detail: `Upload failed: ${res.status}` }));
    throw new Error(error.detail || `Upload failed: ${res.status}`);
  }
  return res.json();
}
```

**Step 3: Create `frontend/src/hooks/use-documents.ts`**

```typescript
import { useState, useEffect, useCallback } from "react";
import { apiGet, apiDelete, apiUpload } from "@/lib/api";
import { supabase } from "@/lib/supabase";
import type { Document } from "@/types";

export function useDocuments() {
  const [documents, setDocuments] = useState<Document[]>([]);
  const [loading, setLoading] = useState(true);

  const fetchDocuments = useCallback(async () => {
    try {
      const docs = await apiGet<Document[]>("/api/documents");
      setDocuments(docs);
    } catch (error) {
      console.error("Failed to fetch documents:", error);
    } finally {
      setLoading(false);
    }
  }, []);

  // Initial fetch
  useEffect(() => {
    fetchDocuments();
  }, [fetchDocuments]);

  // Supabase Realtime subscription for status updates
  useEffect(() => {
    const channel = supabase
      .channel("documents-changes")
      .on(
        "postgres_changes",
        { event: "UPDATE", schema: "public", table: "documents" },
        (payload) => {
          setDocuments((prev) =>
            prev.map((doc) =>
              doc.id === payload.new.id ? { ...doc, ...payload.new } as Document : doc
            )
          );
        }
      )
      .subscribe();

    return () => {
      supabase.removeChannel(channel);
    };
  }, []);

  const uploadDocument = useCallback(async (file: File) => {
    const doc = await apiUpload<Document>("/api/documents", file);
    setDocuments((prev) => [doc, ...prev]);
    return doc;
  }, []);

  const deleteDocument = useCallback(async (documentId: string) => {
    await apiDelete(`/api/documents/${documentId}`);
    setDocuments((prev) => prev.filter((d) => d.id !== documentId));
  }, []);

  return { documents, loading, uploadDocument, deleteDocument, refetchDocuments: fetchDocuments };
}
```

**Step 4: Create `frontend/src/components/documents/file-upload-zone.tsx`**

```tsx
import { useState, useRef, useCallback } from "react";
import { Upload } from "lucide-react";

interface FileUploadZoneProps {
  onUpload: (file: File) => Promise<void>;
  disabled?: boolean;
}

export function FileUploadZone({ onUpload, disabled }: FileUploadZoneProps) {
  const [isDragOver, setIsDragOver] = useState(false);
  const [isUploading, setIsUploading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const inputRef = useRef<HTMLInputElement>(null);

  const handleFile = useCallback(
    async (file: File) => {
      setError(null);
      setIsUploading(true);
      try {
        await onUpload(file);
      } catch (err) {
        setError(err instanceof Error ? err.message : "Upload failed");
      } finally {
        setIsUploading(false);
      }
    },
    [onUpload]
  );

  const handleDrop = useCallback(
    (e: React.DragEvent) => {
      e.preventDefault();
      setIsDragOver(false);
      const file = e.dataTransfer.files[0];
      if (file) handleFile(file);
    },
    [handleFile]
  );

  const handleChange = useCallback(
    (e: React.ChangeEvent<HTMLInputElement>) => {
      const file = e.target.files?.[0];
      if (file) handleFile(file);
      if (inputRef.current) inputRef.current.value = "";
    },
    [handleFile]
  );

  return (
    <div
      onDragOver={(e) => {
        e.preventDefault();
        setIsDragOver(true);
      }}
      onDragLeave={() => setIsDragOver(false)}
      onDrop={handleDrop}
      onClick={() => inputRef.current?.click()}
      className={`flex cursor-pointer flex-col items-center justify-center rounded-lg border-2 border-dashed p-8 transition-colors ${
        isDragOver
          ? "border-primary bg-primary/5"
          : "border-muted-foreground/25 hover:border-muted-foreground/50"
      } ${disabled || isUploading ? "pointer-events-none opacity-50" : ""}`}
    >
      <Upload className="mb-2 h-8 w-8 text-muted-foreground" />
      <p className="text-sm text-muted-foreground">
        {isUploading
          ? "Uploading..."
          : "Drag & drop a file here, or click to browse"}
      </p>
      <p className="mt-1 text-xs text-muted-foreground/70">
        Supported: .txt, .md, .html (max 50MB)
      </p>
      {error && <p className="mt-2 text-sm text-destructive">{error}</p>}
      <input
        ref={inputRef}
        type="file"
        accept=".txt,.md,.html,.htm"
        onChange={handleChange}
        className="hidden"
      />
    </div>
  );
}
```

**Step 5: Create `frontend/src/components/documents/document-list.tsx`**

```tsx
import { Trash2, FileText, Loader2, CheckCircle2, XCircle } from "lucide-react";
import { Button } from "@/components/ui/button";
import type { Document } from "@/types";

interface DocumentListProps {
  documents: Document[];
  onDelete: (id: string) => Promise<void>;
}

function StatusBadge({ status }: { status: Document["status"] }) {
  switch (status) {
    case "pending":
      return (
        <span className="inline-flex items-center gap-1 rounded-full bg-yellow-100 px-2 py-0.5 text-xs text-yellow-800">
          <Loader2 className="h-3 w-3 animate-spin" />
          Pending
        </span>
      );
    case "processing":
      return (
        <span className="inline-flex items-center gap-1 rounded-full bg-blue-100 px-2 py-0.5 text-xs text-blue-800">
          <Loader2 className="h-3 w-3 animate-spin" />
          Processing
        </span>
      );
    case "completed":
      return (
        <span className="inline-flex items-center gap-1 rounded-full bg-green-100 px-2 py-0.5 text-xs text-green-800">
          <CheckCircle2 className="h-3 w-3" />
          Completed
        </span>
      );
    case "failed":
      return (
        <span className="inline-flex items-center gap-1 rounded-full bg-red-100 px-2 py-0.5 text-xs text-red-800">
          <XCircle className="h-3 w-3" />
          Failed
        </span>
      );
  }
}

function formatFileSize(bytes: number): string {
  if (bytes < 1024) return `${bytes} B`;
  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;
  return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;
}

export function DocumentList({ documents, onDelete }: DocumentListProps) {
  if (documents.length === 0) {
    return (
      <div className="py-8 text-center text-sm text-muted-foreground">
        No documents uploaded yet. Upload a file above to get started.
      </div>
    );
  }

  return (
    <div className="divide-y rounded-lg border">
      {documents.map((doc) => (
        <div key={doc.id} className="flex items-center gap-3 p-3">
          <FileText className="h-5 w-5 shrink-0 text-muted-foreground" />
          <div className="min-w-0 flex-1">
            <p className="truncate text-sm font-medium">{doc.filename}</p>
            <div className="flex items-center gap-2 text-xs text-muted-foreground">
              <span>{formatFileSize(doc.file_size)}</span>
              {doc.status === "completed" && (
                <span>{doc.chunk_count} chunks</span>
              )}
              {doc.status === "failed" && doc.error_message && (
                <span className="text-destructive" title={doc.error_message}>
                  {doc.error_message.slice(0, 50)}
                </span>
              )}
            </div>
          </div>
          <StatusBadge status={doc.status} />
          <Button
            variant="ghost"
            size="icon"
            className="h-8 w-8 shrink-0"
            onClick={() => onDelete(doc.id)}
          >
            <Trash2 className="h-4 w-4" />
          </Button>
        </div>
      ))}
    </div>
  );
}
```

**Step 6: Create `frontend/src/pages/documents.tsx`**

```tsx
import { useDocuments } from "@/hooks/use-documents";
import { FileUploadZone } from "@/components/documents/file-upload-zone";
import { DocumentList } from "@/components/documents/document-list";

export function DocumentsPage() {
  const { documents, loading, uploadDocument, deleteDocument } = useDocuments();

  const handleUpload = async (file: File) => {
    await uploadDocument(file);
  };

  return (
    <div className="mx-auto flex h-full max-w-3xl flex-col gap-6 p-6">
      <div>
        <h1 className="text-2xl font-bold">Documents</h1>
        <p className="text-sm text-muted-foreground">
          Upload documents to search during chat conversations.
        </p>
      </div>
      <FileUploadZone onUpload={handleUpload} />
      {loading ? (
        <div className="py-8 text-center text-sm text-muted-foreground">
          Loading documents...
        </div>
      ) : (
        <DocumentList documents={documents} onDelete={deleteDocument} />
      )}
    </div>
  );
}
```

**Validate:** `cd frontend && npx tsc --noEmit` â€” no type errors. Start frontend, navigate to `/documents` â€” page renders with upload zone and empty document list.

**Step 7: Commit**

```bash
git add frontend/src/types/index.ts frontend/src/lib/api.ts frontend/src/hooks/use-documents.ts frontend/src/components/documents/ frontend/src/pages/documents.tsx
git commit -m "feat: add documents page with upload, list, and realtime status"
```

---

## Phase 5: Integration & Cleanup

### Task 16: End-to-End Ingestion + Retrieval Validation

**No files to change â€” manual testing.**

**Step 1: Start both servers**

```bash
# Terminal 1
cd backend && source venv/Scripts/activate && uvicorn app.main:app --reload --port 8000

# Terminal 2
cd frontend && npm run dev
```

**Step 2: Test document upload**

1. Navigate to `/documents`
2. Upload a `.txt` file with some distinctive content (e.g., a paragraph about a specific topic)
3. Verify status changes: pending â†’ processing â†’ completed (via Realtime)
4. Verify chunk count appears

**Step 3: Test retrieval via chat**

1. Navigate to Chat
2. Ask a question about the content you just uploaded
3. The LLM should call the `search_documents` tool and incorporate the document content in its response
4. Check LangSmith traces â€” you should see the tool call and retrieval results

**Step 4: Test document deletion**

1. Navigate to `/documents`
2. Delete the uploaded document
3. Verify it disappears from the list
4. Ask the same question in chat â€” the LLM should no longer find relevant documents

**Step 5: Test error handling**

1. Upload a file with an unsupported MIME type (e.g., `.jpg`) â€” should get a 400 error
2. Check that a failed document shows the error status

**Expected:** Full flow works: upload â†’ process â†’ embed â†’ search via chat â†’ delete.

---

### Task 17: Update PROGRESS.md

**Files:**
- Modify: `PROGRESS.md`

**Step 1: Add Module 2 progress**

```markdown
### Module 2: BYO Retrieval + Memory
- [x] Task 1: Update config for generic LLM providers
- [x] Task 2: Rewrite LLM service (Responses API â†’ Chat Completions)
- [x] Task 3: Update chat router with history loading and tool call loop
- [x] Task 4: Update frontend SSE handler
- [x] Task 5: Database migration â€” drop openai_response_id
- [x] Task 6: End-to-end chat validation
- [x] Task 7: Documents table
- [x] Task 8: Chunks table with pgvector
- [x] Task 9: Supabase Storage bucket setup
- [x] Task 10: Embedding service
- [x] Task 11: Ingestion service (chunk + embed pipeline)
- [x] Task 12: Retrieval service (vector similarity search)
- [x] Task 13: Documents router (upload, list, delete)
- [x] Task 14: Frontend navigation (Chat â†” Documents)
- [x] Task 15: Documents page with upload, list, realtime status
- [x] Task 16: End-to-end ingestion + retrieval validation
```

**Step 2: Commit**

```bash
git add PROGRESS.md
git commit -m "docs: update progress for Module 2 completion"
```

---

## Important Gotchas

1. **Tool call streaming** â€” Tool calls arrive in chunks across multiple SSE events. The `llm_service.py` accumulates them before yielding the complete tool call. The chat router handles execution.

2. **Embedding vector format** â€” Supabase/pgvector expects the embedding as a list of floats. The OpenAI SDK returns this format natively. No conversion needed.

3. **IVFFlat index** â€” The vector index in `005_chunks.sql` uses `lists=100`. With fewer than ~1000 chunks, Postgres may use a sequential scan instead (which is actually faster for small datasets). The index kicks in as data grows.

4. **Supabase Realtime + RLS** â€” For the frontend Realtime subscription to work, RLS must be enabled on `documents` AND the user must be authenticated with Supabase (the anon key session). The `postgres_changes` filter will only deliver rows the user has SELECT access to.

5. **Background task lifecycle** â€” `asyncio.create_task(process_document(...))` runs in the same event loop as FastAPI. If the server restarts mid-processing, the task is lost and the document stays in "processing" status. Acceptable for this course; production would use a task queue.

6. **File encoding** â€” `file_bytes.decode("utf-8")` in the ingestion service assumes UTF-8 text files. Non-UTF-8 files will fail with a clear error. Module 5 adds proper document parsing.

7. **Embedding API key** â€” If `EMBEDDING_API_KEY` is blank, the embedding service will fail. If using OpenAI for embeddings, use your OpenAI key. If using OpenRouter, note that not all OpenRouter models support embeddings â€” check their docs.

8. **OpenRouter tool calling** â€” Not all models on OpenRouter support tool calling. Use a model that does (e.g., `anthropic/claude-sonnet-4`, `openai/gpt-4.1`). If tool calling isn't supported, the LLM will just respond without searching documents.

9. **Supabase Storage upload** â€” The `supabase-py` storage client's `upload()` method takes bytes, not a file handle. We read the full file into memory in the documents router. For very large files, this could be an issue â€” but the 50MB limit keeps it reasonable.
